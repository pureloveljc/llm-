#!/usr/bin/env python3
"""Batch‑transcribe videos in leaf directories using **Hugging Face Transformers**
   with the `openai/whisper‑large‑v3` checkpoint (or any other Whisper ID).

   ─────────────────────────────────────────────────────────────────────────────
   • Generates **plain TXT** and **SRT subtitles with sentence timestamps**
   • Supports **single‑GPU** (e.g. `cuda:1`) **or multi‑GPU** (e.g. `1,2`) loading
   • Long audio handled via the *chunked* algorithm (30‑s windows)

   Usage
   -----
   # 单 GPU：第 1 号卡
   python transcribe_leaf_videos_hf.py /data2/train_data --device cuda:1

   # 多 GPU：第 1 和第 2 号卡一起（显存足够时更快）
   python transcribe_leaf_videos_hf.py /data2/train_data --device 1,2 --batch-size 32

   # CPU 回退
   python transcribe_leaf_videos_hf.py /data2/train_data --device cpu

   Dependencies
   ------------
   pip install -U transformers datasets[audio] accelerate torch tqdm
   sudo apt-get install ffmpeg
"""

from __future__ import annotations

import argparse
import os
import sys
from pathlib import Path
from typing import List, Optional, Set, Tuple

import torch
from tqdm import tqdm
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline

# --------------------------------------------------------------------------------------
# Config
# --------------------------------------------------------------------------------------
VIDEO_EXTENSIONS: Set[str] = {
    ".mp4", ".m4v", ".mov", ".mkv", ".avi", ".flv", ".wmv", ".webm", ".mpg", ".mpeg", ".mpga", ".wav",
}
DEFAULT_MODEL_ID = "openai/whisper-large-v3"

# --------------------------------------------------------------------------------------
# Helpers
# --------------------------------------------------------------------------------------

def find_leaf_video_files(root: Path) -> List[Path]:
    """Return sorted list of video files located in **leaf** directories (no sub‑dirs)."""
    leaf_videos: List[Path] = []
    for current, dirs, files in os.walk(root):
        if not dirs:
            for fname in files:
                if Path(fname).suffix.lower() in VIDEO_EXTENSIONS:
                    leaf_videos.append(Path(current) / fname)
    return sorted(leaf_videos)


def parse_selection(sel: str, total: int) -> List[int]:
    """Convert selection string → 0‑based indices list."""
    sel = sel.strip().lower()
    if sel in {"all", "*", ""}:
        return list(range(total))

    idx_set: Set[int] = set()
    for part in sel.split(','):
        part = part.strip()
        if not part:
            continue
        if '-' in part:
            a, b = part.split('-', 1)
            start, end = int(a) - 1, int(b) - 1
            if start < 0 or start > end:
                raise ValueError(f"无效区间 '{part}'")
            idx_set.update(range(start, end + 1))
        else:
            idx = int(part) - 1
            if idx < 0:
                raise ValueError(f"无效索引 '{part}'")
            idx_set.add(idx)
    return sorted(i for i in idx_set if i < total)

# --------------------------------------------------------------------------------------
# Timestamp util
# --------------------------------------------------------------------------------------

def srt_time(sec: float) -> str:
    ms = int(round(sec * 1000))
    h = ms // 3_600_000
    m = (ms % 3_600_000) // 60_000
    s = (ms % 60_000) // 1000
    ms %= 1000
    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"

# --------------------------------------------------------------------------------------
# Pipeline loader
# --------------------------------------------------------------------------------------

def load_pipeline(model_id: str, device_arg: str, chunk_len: int, batch_size: int):
    """Return a HuggingFace *pipeline* with optional multi‑GPU support."""

    # ---------- interpret device_arg ----------
    if device_arg == "cpu":
        device_map = None
        target_device = "cpu"
    elif "," in device_arg:  # multi‑GPU, e.g. "1,2"
        os.environ["CUDA_VISIBLE_DEVICES"] = device_arg  # restrict visible GPUs
        device_map = "auto"  # let Accelerate split across all visible cards
        target_device = "cuda"
    else:  # single GPU string like "cuda" or "cuda:1"
        device_map = None
        target_device = device_arg if device_arg.startswith("cuda") else "cuda"

    torch_dtype = torch.float16 if target_device.startswith("cuda") else torch.float32

    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_id,
        torch_dtype=torch_dtype,
        low_cpu_mem_usage=True,
        use_safetensors=True,
        device_map=device_map,  # None for single‑GPU/CPU, "auto" for multi‑GPU
    )

    # If single device, move entire model
    if device_map is None and target_device != "cpu":
        model.to(target_device)

    processor = AutoProcessor.from_pretrained(model_id)

    return pipeline(
        "automatic-speech-recognition",
        model=model,
        tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor,
        chunk_length_s=chunk_len,
        batch_size=batch_size,
        torch_dtype=torch_dtype,
        device=0 if target_device.startswith("cuda") else -1,  # pipeline arg
    )

# --------------------------------------------------------------------------------------
# Transcribe single file
# --------------------------------------------------------------------------------------

def transcribe_file(pipe, path: Path, lang: Optional[str]):
    kw = {"language": lang} if lang else {}
    return pipe(str(path), return_timestamps=True, generate_kwargs=kw)

# --------------------------------------------------------------------------------------
# Main
# --------------------------------------------------------------------------------------

def main():
    p = argparse.ArgumentParser(description="Transcribe videos → TXT + SRT (HF Whisper)")
    p.add_argument("root_dir", type=Path, help="根目录路径")
    p.add_argument("--select", default="all", help="处理哪些文件，如 '1-5,8' or all")
    p.add_argument("--model-id", default=DEFAULT_MODEL_ID, help="Hugging Face 模型 ID")
    p.add_argument("--device", default=None, help="cuda, cuda:1 or '1,2' for multi")
    p.add_argument("--language", default=None, help="提示音频语言，如 zh")
    p.add_argument("--out-dir", type=Path, default=Path("./output_tts"))
    p.add_argument("--chunk-len", type=int, default=30)
    p.add_argument("--batch-size", type=int, default=16)
    p.add_argument("--overwrite", action="store_true")
    args = p.parse_args()

    # Decide device default
    device_arg = args.device or ("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Loading model {args.model_id} on {device_arg} …")
    pipe = load_pipeline(args.model_id, device_arg, args.chunk_len, args.batch_size)

    videos = find_leaf_video_files(args.root_dir)
    if not videos:
        print("未找到视频文件，结束。"); return

    sel_idx = parse_selection(args.select, len(videos))
    if not sel_idx:
        print("没有任何视频被选中，结束。"); return

    print(f"将处理 {len(sel_idx)} 个文件 …\n")

    for i_idx in tqdm(sel_idx, desc="Transcribing"):
        vid = videos[i_idx]
        out_dir = args.out_dir or vid.parent
        out_dir.mkdir(parents=True, exist_ok=True)
        txt_f = out_dir / f"{vid.stem}.txt"
        srt_f = out_dir / f"{vid.stem}.srt"
        if not args.overwrite and txt_f.exists() and srt_f.exists():
            tqdm.write(f"[SKIP] {vid} 已存在输出文件。")
            continue
        try:
            res = transcribe_file(pipe, vid, args.language)
            txt_f.write_text(res["text"].strip(), encoding="utf-8")
            with srt_f.open("w", encoding="utf-8") as srt:
                for n, c in enumerate(res.get("chunks", []), 1):
                    b, e = c["timestamp"]
                    srt.write(f"{n}\n{srt_time(b)} --> {srt_time(e)}\n{c['text'].strip()}\n\n")
            tqdm.write(f"[OK] {vid.name} → TXT+SRT")
        except Exception as exc:
            tqdm.write(f"[ERROR] {vid.name}: {exc}")

if __name__ == "__main__":
    main()
